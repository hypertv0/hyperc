import requests
from bs4 import BeautifulSoup
import re
import concurrent.futures
import time
import random

# --- AYARLAR ---
MAX_WORKERS = 10  # HÄ±z iÃ§in iÅŸ parÃ§acÄ±ÄŸÄ± sayÄ±sÄ± (Ã‡ok artÄ±rÄ±rsan IP ban yersin)
MAX_PAGES = 500   # Her kategori iÃ§in taranacak maksimum sayfa sayÄ±sÄ± (Sonsuz dÃ¶ngÃ¼yÃ¼ engellemek iÃ§in gÃ¼venlik sÄ±nÄ±rÄ±)
RETRY_COUNT = 3   # BaÅŸarÄ±sÄ±z istekleri kaÃ§ kez denesin

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Referer": "https://google.com"
}

def get_current_domain():
    try:
        url = "https://raw.githubusercontent.com/Kraptor123/domainListesi/refs/heads/main/eklenti_domainleri.txt"
        response = requests.get(url, timeout=10)
        content = response.text
        for line in content.splitlines():
            if line.strip().startswith("DiziPal"):
                domain = line.split(":")[-1].strip()
                if not domain.startswith("http"):
                    domain = "https://" + domain
                print(f"[+] GÃ¼ncel Domain: {domain}")
                return domain
    except Exception as e:
        print(f"[-] Domain bulunamadÄ±, varsayÄ±lan kullanÄ±lÄ±yor: {e}")
    return "https://dizipal1217.com"

BASE_URL = get_current_domain()
HEADERS["Referer"] = BASE_URL + "/"

def get_iframe_source(url):
    """Linkin iÃ§indeki m3u8 dosyasÄ±nÄ± bulur."""
    for _ in range(RETRY_COUNT):
        try:
            res = requests.get(url, headers=HEADERS, timeout=10)
            if res.status_code != 200: return None
            
            soup = BeautifulSoup(res.text, 'html.parser')
            iframe = soup.select_one('.series-player-container iframe') or soup.select_one('div#vast_new iframe')
            
            if iframe:
                src = iframe.get('src')
                if src:
                    # Iframe'e git
                    iframe_res = requests.get(src, headers={"Referer": BASE_URL}, timeout=10)
                    match = re.search(r'file:"([^"]+)"', iframe_res.text)
                    if match:
                        return match.group(1)
            break # BaÅŸarÄ±lÄ±ysa dÃ¶ngÃ¼den Ã§Ä±k
        except:
            time.sleep(1) # Hata olursa 1sn bekle tekrar dene
    return None

def process_item(item, category_name):
    """Tek bir iÃ§eriÄŸi iÅŸler."""
    try:
        # Site yapÄ±sÄ±na gÃ¶re baÅŸlÄ±k ve link seÃ§icileri
        title_tag = item.select_one('.title') or item.select_one('h5') or item.select_one('.name')
        link_tag = item.select_one('a')
        img_tag = item.select_one('img')
        
        if not title_tag or not link_tag:
            return None
            
        title = title_tag.text.strip()
        link = link_tag.get('href')
        poster = img_tag.get('src') if img_tag else ""
        
        if not link.startswith("http"):
            link = BASE_URL + link

        stream_url = get_iframe_source(link)
        
        if stream_url:
            # M3U Entry
            m3u = f'#EXTINF:-1 group-title="{category_name}" tvg-logo="{poster}", {title}\n'
            m3u += f'#EXTVLCOPT:http-referrer={BASE_URL}/\n'
            m3u += f'#EXTHTTP:{{"Referer": "{BASE_URL}/"}}\n'
            m3u += f'{stream_url}\n'
            return m3u
    except:
        return None
    return None

def scrape_category_pages(base_path, category_name):
    """Bir kategorinin TÃœM sayfalarÄ±nÄ± tarar."""
    print(f"\nğŸš€ KATEGORÄ° BAÅLIYOR: {category_name}")
    category_m3u_entries = []
    
    page = 1
    empty_streak = 0 # BoÅŸ sayfa sayacÄ±

    while page <= MAX_PAGES:
        # Sayfa URL yapÄ±sÄ±nÄ± oluÅŸtur
        if page == 1:
            target_url = f"{BASE_URL}{base_path}"
        else:
            # Genellikle yapÄ± /page/2 ÅŸeklindedir
            target_url = f"{BASE_URL}{base_path}/page/{page}"
        
        try:
            res = requests.get(target_url, headers=HEADERS, timeout=15)
            
            # EÄŸer 404 dÃ¶nerse veya anasayfaya yÃ¶nlendirirse kategori bitmiÅŸtir
            if res.status_code == 404 or res.url == BASE_URL:
                print(f"   ğŸ›‘ Sayfa {page} bulunamadÄ±. Kategori bitti.")
                break

            soup = BeautifulSoup(res.text, 'html.parser')
            
            # Ä°Ã§erikleri bul
            items = soup.select('div.episode-item') + soup.select('article.type2 ul li')
            
            if not items:
                print(f"   âš ï¸ Sayfa {page} boÅŸ. (Ä°Ã§erik bulunamadÄ±)")
                break
                
            print(f"   ğŸ“„ Sayfa {page} taranÄ±yor... ({len(items)} iÃ§erik)")
            
            # Paralel iÅŸlem ile linkleri Ã§ek
            current_page_entries = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(process_item, item, category_name) for item in items]
                for future in concurrent.futures.as_completed(futures):
                    result = future.result()
                    if result:
                        current_page_entries.append(result)
            
            if len(current_page_entries) == 0:
                print("   âŒ Bu sayfadan oynatÄ±labilir link Ã§Ä±kmadÄ±.")
                # Link Ã§Ä±kmasa bile sayfada iÃ§erik vardÄ±, o yÃ¼zden devam et
            else:
                category_m3u_entries.extend(current_page_entries)
                print(f"   âœ… Sayfa {page} tamamlandÄ±. {len(current_page_entries)} link eklendi.")

            page += 1
            # IP Ban yememek iÃ§in sayfa geÃ§iÅŸlerinde rastgele bekleme
            time.sleep(random.uniform(0.5, 1.5))
            
        except Exception as e:
            print(f"   ğŸ”¥ Hata (Sayfa {page}): {e}")
            break

    return category_m3u_entries

def main():
    # Kategori Listesi
    categories = [
        ("/diziler/son-bolumler", "Son BÃ¶lÃ¼mler"),
        ("/filmler", "Filmler"), # "Yeni Filmler" yerine genel "Filmler" daha Ã§ok iÃ§erik verir
        ("/diziler", "Diziler"),
        ("/koleksiyon/netflix", "Netflix"),
        ("/koleksiyon/exxen", "Exxen"),
        ("/koleksiyon/blutv", "BluTV"),
        ("/koleksiyon/disney", "Disney+"),
        ("/koleksiyon/amazon-prime", "Amazon Prime"),
        ("/koleksiyon/gain", "Gain"),
        ("/tur/mubi", "Mubi")
    ]
    
    # DosyayÄ± sÄ±fÄ±rla ve baÅŸlÄ±ÄŸÄ± yaz
    with open("dizipal.m3u", "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")

    total_added = 0
    
    for path, name in categories:
        entries = scrape_category_pages(path, name)
        
        # Her kategori bittiÄŸinde dosyaya ekle (RAM ÅŸiÅŸmesin diye)
        if entries:
            with open("dizipal.m3u", "a", encoding="utf-8") as f:
                f.writelines(entries)
            total_added += len(entries)
            print(f"ğŸ’¾ {name} kaydedildi. (Toplam: {total_added})")
        
    print(f"\nğŸ‰ TÃœM Ä°ÅLEM BÄ°TTÄ°! Toplam {total_added} iÃ§erik 'dizipal.m3u' dosyasÄ±na kaydedildi.")

if __name__ == "__main__":
    main()
